{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is a Neural Network anyway?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset of dosages , ( low , medium and high), we can't fit a straight line to make an accurate prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/TcuaKgc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural network can effectively fit a squiggle for the data, even for complicated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "- When you build a NN you have to decide what activation function you may use ( for learning : sigmoid , in practice : ReLU)\n",
    "- When you build a NN you make a guess on how many hidden layers and input you will use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this as an example of a NN :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://miro.medium.com/max/875/1*dypctO_eJnrXqX6JUa5MDA.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where a Hidden Layer can be defined as:\n",
    "\n",
    "- $ H*i = W*{ij} \\ * I\\_{i} + B$\n",
    "\n",
    "'W' represents the weight and is every connection between the input layer and the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Perceptron](https://en.wikipedia.org/wiki/Perceptron) can be defined as single layer NN , it's based on an artificial neuron, it can take several binary inputs $x_1,x_2,...,$ and produce a single binary output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Hidden node , the math processed is a 'weighted sum' , like so:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/hur6mHW.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork code and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a NN from scratch !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights , the heart of a NN\n",
    "\n",
    "The weights are one of the most important parameters of a NN, they are basically the link between the different layers of the NN , and they help feeding the error , calculating the prediction , and it's basically what helps improving the NN , the weight link can be represented as matrices , so we will use that to implement them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the input and hidden layers $W_{input \\ hiden}$ , of $ dim = (​\n",
    "  input_nodes​ \\ x\\ hidden_nodes ) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the hidden and the output layers $W_{hidden\\_output}$ , of $dim = (​\n",
    " output\\_nodes ​  \\ x\\ hidden\\_nodes )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the link weights should be small and\n",
    "random , unlike in logistic regression where we can simply set the initial weights as 0 , in a NN this will NOT work (if you initial the hidden units as 0 , then all of them will be symmetric matrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw=\\begin{bmatrix}\n",
    "u & v \\\\\n",
    "u & v \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both hidden units (eg: $a_1$ and $a_2$ ) will have the same influence on the output layer , after one iteration or many , both will remain symmetric , and no mather how many times you run it will compute the same function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn by refining their link weights. This is guided by the error​: the difference between the right answer given by the training data and their actual output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ending up in extreme parts of the correspoint function, it would be helpful to set $W$ as small numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09532906, -0.24523873, -0.38213995]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1,3)) - 0.5 # define also negative weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Gradient descent​ is a really good way of working out the minimum of a function, and it really works well when that function is so complex and difficult that we couldn’t easily\n",
    "work it out mathematically using algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the error as a matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ error*{hidden} = W^T * {hidden\\_output} \\ * error\\_{output} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = mx + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $m$ to change based on the error :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta m = \\alpha x\\ * error $$\n",
    "\n",
    "$$ \\Delta B = \\alpha error $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation with gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is defined as a \" first-order iterative optimization algorithm for finding a local minimum of a differentiable function\", that is , in more human terms , a way of finding the steepest descent to a minimum .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Neural Networks , we are trying to minimize the NN error, defining the weight :\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W\\_{jk}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta W_{jk} = \\alpha E_k (O_k) + \\sigma(O_k) (1-\\sigma(O_k)) (O_j)^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us find the change in error as the weight links change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving a neural network means reducing this error ­ by changing those weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Choosing the right weights directly is too difficult. An alternative approach is to\n",
    "iteratively improve the weights by descending the <b> error </b> function, taking small steps.\n",
    "Each step is taken in the direction of the greatest downward slope from your current\n",
    "position. This is called ​ g\n",
    "radient descent​\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good recommendation is to rescale inputs into the range 0.0 to 1.0. Some will add a small\n",
    "offset to the inputs, like 0.01, just to avoid having zero inputs which are troublesome because\n",
    "they kill the learning ability by zeroing the weight update expression by setting that ​ oj=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should try to avoid saturating a NN keeping the inputs small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to actually propagate weights ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Bias Neural Network Class\n",
    "from scipy.special import expit  # expit reprents the sigmoid function\n",
    "\n",
    "class neuralNetwork:\n",
    "    #constructor body\n",
    "\n",
    "    def __init__(self,input_nodes,hidden_nodes,output_nodes,alpha):\n",
    "        \n",
    "        # defining a sigmoid or activation only once so that it can be referenced several times\n",
    "        self.sigmoid = lambda x: expit(x)\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # weights\n",
    "        self.W_ih = np.random.normal(0.0, pow(self.input_nodes, -0.5), (self.hidden_nodes, self.input_nodes))\n",
    "        self.W_ho = np.random.normal(0.0, pow(self.hidden_nodes, -0.5), (self.output_nodes, self.hidden_nodes))\n",
    "\n",
    "\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    def train(self,inputs_list,target_variable): # target_variable represent 'y' or the variable you want to predict\n",
    "        \n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        outputs,hidden_outputs = self.feedforward(inputs_list)\n",
    "        target = np.array(target_variable, ndmin=2).T\n",
    "        \n",
    "        #print(target_variable)\n",
    "        \n",
    "        # Find the error e = (t - o) - matrix subtraction \n",
    "        output_errors = target - outputs\n",
    "\n",
    "        # getting the hidden error ( W.T * e) - weights hidden - outputs\n",
    "        W_ho_T= (self.W_ho).T # transpose the weight matrix\n",
    "                \n",
    "        # find hidden layer error\n",
    "        hidden_errors = np.dot(W_ho_T,output_errors)\n",
    "        \n",
    "        # NOTE : FEED FORWARD already mapped the outputs to the sigmoid function\n",
    "        # implementing the delta weight change between the hidden layer and the output layer\n",
    "        \n",
    "        gradient = lambda x : x * (1-x)\n",
    "        \n",
    "        gradient_Who = gradient(outputs)\n",
    "        gradient_Wih = gradient(hidden_outputs)\n",
    "        \n",
    "        # link update between hidden layers and the output layers\n",
    "        self.W_ho += self.alpha * np.dot(output_errors*gradient_Who, np.transpose(hidden_outputs))\n",
    "        \n",
    "        # link update between input layers and the hidden layers\n",
    "        self.W_ih += self.alpha * np.dot(hidden_errors*gradient_Wih,np.transpose(inputs))\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # receive inputs -> generate hidden ! outputs\n",
    "    # feeding the information forward to the NN\n",
    "    def feedforward(self,inputs_list):\n",
    "        \n",
    "        # before predicting we need to turn the input into a 2D\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "\n",
    "    \n",
    "        # after that , we do a matrix multiplication of the inputs with the weights \n",
    "        # TODO add Bias\n",
    "        hidden_inputs = np.dot(self.W_ih,inputs)\n",
    "        # calculate the sigmoid function\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
    "        \n",
    "        # take the hidden outputs and do the matrix product between the hidden layer and the output layer\n",
    "\n",
    "        final_inputs = np.dot(self.W_ho,hidden_outputs)\n",
    "        \n",
    "\n",
    "        outputs = self.sigmoid(final_inputs)\n",
    "\n",
    "        \n",
    "        return outputs,hidden_outputs\n",
    "\n",
    "\n",
    "# number of input, hidden and output nodes\n",
    "# input_nodes =2\n",
    "# hidden_nodes = 2\n",
    "# output_nodes = 1\n",
    "# create instance of neural network\n",
    "\n",
    "#\n",
    "\n",
    "# n = neuralNetwork(2,2,2,0.5)\n",
    "\n",
    "# input_nodes = [1,0]\n",
    "# targets = [1,0]\n",
    "# n.train(input_nodes,targets)\n",
    "\n",
    "# class instance of the neural network       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data.\n",
    "X = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "# The labels for the training data.\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5000):\n",
    "#     for j in range(4):\n",
    "#         x = random.choice(X[j])\n",
    "#         print(x[j],y[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = neuralNetwork(2,2,1,0.01)\n",
    "\n",
    "# #random.shuffle(training_data)\n",
    "# # TODO : fix this damn thing\n",
    "# for i in range(5000):\n",
    "#     for j in range(4):\n",
    "#         n.train(X[j],y[j])\n",
    "        \n",
    "# print(n.feedforward([0,1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset of Handwritten Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'mnist_train_100.csv' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget   -nc https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'mnist_train_100.csv' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget  -nc https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('mnist_train_100.csv','r') # r parameter is optional\n",
    "data_list = data_file.readlines() \n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b560b81f10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_value = data_list[0].split(',')\n",
    "\n",
    "img_array = np.asfarray(all_value[1:]).reshape(28,28) # return array convert to float type\n",
    "\n",
    "plt.imshow(img_array,cmap='inferno',interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the MNIST Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve worked out how to get data out of the MNIST data files and disentangle it so we can\n",
    "make sense of it, and visualise it too. We want to train our neural network with this data, but we\n",
    "need to think just a little about preparing this data before we throw it at our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to rescale the input color values from 0 to 255 , to a smaller range , say (0.01-1.0) avoiding 0 so they don't transform weights into ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = ( np.asfarray(all_value[1:]) )/ (255 * 0.99) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.02188354 0.08130125 0.08130125 0.08130125\n",
      " 0.50910873 0.54872054 0.70320658 0.11299069 0.66755595 1.02010101\n",
      " 0.98841157 0.51306991 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.12883541 0.1526025  0.38235096 0.62002179\n",
      " 0.68340067 1.01217865 1.01217865 1.01217865 1.01217865 1.01217865\n",
      " 0.9012656  0.69132303 1.01217865 0.96860566 0.78243018 0.26351555\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.20409784\n",
      " 0.95276094 1.01217865 1.01217865 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 1.00425629 0.37838978 0.3348168\n",
      " 0.3348168  0.2318261  0.16448604 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.08130125 0.87749851 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 1.01217865 0.79431373 0.73093484\n",
      " 0.98841157 0.96464448 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.32689443 0.62794415 0.43384631 1.01217865\n",
      " 1.01217865 0.82204199 0.05357298 0.01       0.18033076 0.62002179\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.06545653 0.01396118 0.62002179 1.01217865 0.36650624\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.56060408 1.01217865 0.76262428 0.01792236 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.05357298\n",
      " 0.76262428 1.01217865 0.28728263 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.14864132 0.96464448\n",
      " 0.9012656  0.64378887 0.43780749 0.01396118 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.33085561 0.9606833  1.01217865\n",
      " 1.01217865 0.48138047 0.10902951 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.18825312 0.74677956 1.01217865 1.01217865\n",
      " 0.60417706 0.11695187 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.07337889 0.37838978 1.00821747 1.01217865 0.75074074\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.99633393 1.01217865 0.99633393 0.26351555 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.1922143  0.52495346 0.73489602 1.01217865\n",
      " 1.01217865 0.82996435 0.01792236 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.16448604 0.5962547\n",
      " 0.91711032 1.01217865 1.01217865 1.01217865 1.00029511 0.73093484\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.10506833 0.46157457 0.88542088 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 0.80619727 0.31897207 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.10110715 0.27143791 0.85373143 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 0.79431373 0.33085561 0.01792236\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.08130125 0.68736185\n",
      " 0.87749851 1.01217865 1.01217865 1.01217865 1.01217865 0.78243018\n",
      " 0.32689443 0.04565062 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.22786492 0.69132303 0.90522678 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 0.97652803 0.536837   0.05357298 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.54872054 1.01217865\n",
      " 1.01217865 1.01217865 0.84977025 0.54475936 0.53287582 0.07337889\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01      ]\n"
     ]
    }
   ],
   "source": [
    "print(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a label '5' the output should be:\n",
    "\n",
    "Example \"5\" : \n",
    "`\n",
    "[0.01, \n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.99,\n",
    "0.01,\n",
    "0.01,\n",
    "001,\n",
    "0.01,\n",
    "0.01]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the target matrix:\n",
    "\n",
    "output_nodes=10\n",
    "\n",
    "target = np.zeros(output_nodes) + 0.01\n",
    "target[int(all_value[0])] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, 0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "784 input nodes = 28 x 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input , hidden and output nodes\n",
    "\n",
    "\n",
    "input_nodes= 784\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10 # 10 labels\n",
    "\n",
    "#learning rate \n",
    "alpha= 0.3\n",
    "\n",
    "# creating an instance of the neural network\n",
    "\n",
    "n = neuralNetwork(input_nodes, hidden_nodes,output_nodes,alpha)\n",
    "\n",
    "# loading the mnist training data\n",
    "\n",
    "training_data_file = open('mnist_train_100.csv','r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    \n",
    "    all_values = record.split(',')\n",
    "    \n",
    "    #scale and shift the inputs\n",
    "    \n",
    "    inputs= np.asfarray(all_values[1:])/ (255*0.99) + 0.01\n",
    "    \n",
    "    # create the target output values\n",
    "    \n",
    "    targets = np.zeros(output_nodes) + 0.01\n",
    "    # all values[0] is the target label for this record\n",
    "    \n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "    n.train(inputs,targets)\n",
    "    pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'mnist_test_10.csv' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_test_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = open('mnist_test_10.csv','r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "all_values = test_data_list[0].split(',')\n",
    "print(all_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b560bf2370>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "img_array = np.asfarray(all_values[1:]).reshape(28,28) # return array convert to float type\n",
    "\n",
    "plt.imshow(img_array,cmap='inferno',interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13225022],\n",
       "       [0.13843084],\n",
       "       [0.05543291],\n",
       "       [0.17047119],\n",
       "       [0.16416345],\n",
       "       [0.05480707],\n",
       "       [0.04626906],\n",
       "       [0.38340048],\n",
       "       [0.13340776],\n",
       "       [0.09064391]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.feedforward((np.asfarray(all_values[1:])/255.0 * 0.99) +0.01)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 correct label\n",
      "7 network output:\n",
      "2 correct label\n",
      "3 network output:\n",
      "1 correct label\n",
      "1 network output:\n",
      "0 correct label\n",
      "0 network output:\n",
      "4 correct label\n",
      "4 network output:\n",
      "1 correct label\n",
      "1 network output:\n",
      "4 correct label\n",
      "4 network output:\n",
      "9 correct label\n",
      "3 network output:\n",
      "5 correct label\n",
      "1 network output:\n",
      "9 correct label\n",
      "7 network output:\n"
     ]
    }
   ],
   "source": [
    "# test the neural network\n",
    "\n",
    "\n",
    "# scoreboard to test the neural network\n",
    "\n",
    "\n",
    "scoreboard = []\n",
    "\n",
    "# go through the records in the test dataset\n",
    "\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    \n",
    "    all_values=record.split(',')\n",
    "    \n",
    "    # correct answer is first value\n",
    "    \n",
    "    correct_label = int(all_values[0])\n",
    "    \n",
    "    print(correct_label,\"correct label\")\n",
    "    \n",
    "    #scale and shift the inputs\n",
    "    \n",
    "    inputs = (np.asfarray(all_values[1:]))/(255.0 * 0.99) + 0.01\n",
    "    \n",
    "    #feed forward \n",
    "    \n",
    "    outputs= n.feedforward(inputs)[0]\n",
    "    \n",
    "    label = np.argmax(outputs)\n",
    "    \n",
    "    print(label,'network output:')\n",
    "    \n",
    "    if(label==correct_label):\n",
    "        scoreboard.append(1)\n",
    "    else:\n",
    "        scoreboard.append(0)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input , hidden and output nodes\n",
    "\n",
    "\n",
    "input_nodes= 784\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10 # 10 labels\n",
    "\n",
    "#learning rate \n",
    "alpha= 0.3\n",
    "\n",
    "# creating an instance of the neural network\n",
    "\n",
    "n = neuralNetwork(input_nodes, hidden_nodes,output_nodes,alpha)\n",
    "\n",
    "# loading the mnist training data\n",
    "\n",
    "training_data_file = open('mnist_train_100.csv','r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    \n",
    "    all_values = record.split(',')\n",
    "    \n",
    "    #scale and shift the inputs\n",
    "    \n",
    "    inputs= np.asfarray(all_values[1:])/ (255*0.99) + 0.01\n",
    "    \n",
    "    # create the target output values\n",
    "    \n",
    "    targets = np.zeros(output_nodes) + 0.01\n",
    "    # all values[0] is the target label for this record\n",
    "    \n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "    n.train(inputs,targets)\n",
    "    pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (np.asfarray(all_values[1:]))/(255.0 * 0.99) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(scoreboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.6\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scoreboard)\n",
    "print (\"performance = \", scorecard_array.sum() /\n",
    "scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the larger dataset for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input , hidden and output nodes\n",
    "\n",
    "\n",
    "input_nodes= 784\n",
    "\n",
    "\n",
    "\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10 # 10 labels\n",
    "\n",
    "#learning rate \n",
    "alpha= 0.3\n",
    "\n",
    "# creating an instance of the neural network\n",
    "\n",
    "n = neuralNetwork(input_nodes, hidden_nodes,output_nodes,alpha)\n",
    "\n",
    "# loading the mnist training data\n",
    "\n",
    "training_data_file = open('Data/mnist_train.csv','r')\n",
    "next(training_data_file) # skip header of\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()\n",
    "\n",
    "# train the neural network\n",
    "\n",
    "for record in training_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    \n",
    "    all_values = record.split(',')\n",
    "    \n",
    "    #scale and shift the inputs\n",
    "    \n",
    "    inputs= np.asfarray(all_values[1:])/ (255*0.99) + 0.01\n",
    "    \n",
    "    # create the target output values\n",
    "    \n",
    "    targets = np.zeros(output_nodes) + 0.01\n",
    "    # all values[0] is the target label for this record\n",
    "    \n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    \n",
    "    n.train(inputs,targets)\n",
    "    pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = open('Data/mnist_test.csv','r')\n",
    "next(test_data_file)\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_data_list) # 10 000 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the neural network\n",
    "\n",
    "\n",
    "# scoreboard to test the neural network\n",
    "\n",
    "\n",
    "scoreboard = []\n",
    "\n",
    "# go through the records in the test dataset\n",
    "\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    \n",
    "    all_values=record.split(',')\n",
    "    \n",
    "    # correct answer is first value\n",
    "    \n",
    "    correct_label = int(all_values[0])\n",
    "    \n",
    "    \n",
    "    #scale and shift the inputs\n",
    "    \n",
    "    inputs = (np.asfarray(all_values[1:]))/(255.0 * 0.99) + 0.01\n",
    "    \n",
    "    #feed forward \n",
    "    \n",
    "    outputs= n.feedforward(inputs)[0]\n",
    "    \n",
    "    label = np.argmax(outputs)\n",
    "        \n",
    "    if(label==correct_label):\n",
    "        scoreboard.append(1)\n",
    "    else:\n",
    "        scoreboard.append(0)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.9464\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scoreboard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n",
    "\n",
    "# it works !!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "265a90865ad91d447cfa81a2dfc56a75477dab88c535fc37f696ed4f589bcec7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
