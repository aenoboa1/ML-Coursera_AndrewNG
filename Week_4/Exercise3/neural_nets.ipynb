{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is a Neural Network anyway?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset of dosages , ( low , medium and high), we can't fit a straight line to make an accurate prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/TcuaKgc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural network can effectively fit a squiggle for the data, even for complicated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "- When you build a NN you have to decide what activation function you may use ( for learning : sigmoid , in practice : ReLU)\n",
    "- When you build a NN you make a guess on how many hidden layers and input you will use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this as an example of a NN :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://miro.medium.com/max/875/1*dypctO_eJnrXqX6JUa5MDA.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where a Hidden Layer can be defined as:\n",
    "\n",
    "- $ H*i = W*{ij} \\ * I\\_{i} + B$\n",
    "\n",
    "'W' represents the weight and is every connection between the input layer and the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Perceptron](https://en.wikipedia.org/wiki/Perceptron) can be defined as single layer NN , it's based on an artificial neuron, it can take several binary inputs $x_1,x_2,...,$ and produce a single binary output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Hidden node , the math processed is a 'weighted sum' , like so:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/hur6mHW.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork code and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a NN from scratch !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights , the heart of a NN\n",
    "\n",
    "The weights are one of the most important parameters of a NN, they are basically the link between the different layers of the NN , and they help feeding the error , calculating the prediction , and it's basically what helps improving the NN , the weight link can be represented as matrices , so we will use that to implement them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the input and hidden layers $W_{input \\ hiden}$ , of $ dim = (​\n",
    "  input_nodes​ \\ x\\ hidden_nodes ) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the hidden and the output layers $W_{hidden\\_output}$ , of $dim = (​\n",
    " output\\_nodes ​  \\ x\\ hidden\\_nodes )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the link weights should be small and\n",
    "random , unlike in logistic regression where we can simply set the initial weights as 0 , in a NN this will NOT work (if you initial the hidden units as 0 , then all of them will be symmetric matrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw=\\begin{bmatrix}\n",
    "u & v \\\\\n",
    "u & v \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both hidden units (eg: $a_1$ and $a_2$ ) will have the same influence on the output layer , after one iteration or many , both will remain symmetric , and no mather how many times you run it will compute the same function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn by refining their link weights. This is guided by the error​: the difference between the right answer given by the training data and their actual output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ending up in extreme parts of the correspoint function, it would be helpful to set $W$ as small numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12133093,  0.34539028, -0.14276052]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1,3)) - 0.5 # define also negative weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Gradient descent​ is a really good way of working out the minimum of a function, and it really works well when that function is so complex and difficult that we couldn’t easily\n",
    "work it out mathematically using algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the error as a matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ error*{hidden} = W^T * {hidden\\_output} \\ * error\\_{output} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = mx + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $m$ to change based on the error :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta m = \\alpha x\\ * error $$\n",
    "\n",
    "$$ \\Delta B = \\alpha error $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation with gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is defined as a \" first-order iterative optimization algorithm for finding a local minimum of a differentiable function\", that is , in more human terms , a way of finding the steepest descent to a minimum .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Neural Networks , we are trying to minimize the NN error, defining the weight :\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W\\_{jk}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta W_{jk} = \\alpha E_k (O_k) + \\sigma(O_k) (1-\\sigma(O_k)) (O_j)^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us find the change in error as the weight links change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving a neural network means reducing this error ­ by changing those weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Choosing the right weights directly is too difficult. An alternative approach is to\n",
    "iteratively improve the weights by descending the <b> error </b> function, taking small steps.\n",
    "Each step is taken in the direction of the greatest downward slope from your current\n",
    "position. This is called ​ g\n",
    "radient descent​\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good recommendation is to rescale inputs into the range 0.0 to 1.0. Some will add a small\n",
    "offset to the inputs, like 0.01, just to avoid having zero inputs which are troublesome because\n",
    "they kill the learning ability by zeroing the weight update expression by setting that ​ oj=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should try to avoid saturating a NN keeping the inputs small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to actually propagate weights ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Bias Neural Network Class\n",
    "from scipy.special import expit  # expit reprents the sigmoid function\n",
    "\n",
    "class neuralNetwork:\n",
    "    #constructor body\n",
    "\n",
    "    def __init__(self,input_nodes,hidden_nodes,output_nodes,alpha):\n",
    "        \n",
    "        # defining a sigmoid or activation only once so that it can be referenced several times\n",
    "        self.sigmoid = lambda x: expit(x)\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # weights\n",
    "\n",
    "        self.W_ih =  np.array(np.random.random((self.hidden_nodes,self.input_nodes))) - 0.5 # needs to be array\n",
    "        self.W_ho = np.array(np.random.random((self.output_nodes,self.hidden_nodes))) - 0.5 \n",
    "\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    def train(self,inputs_list,target_variable): # target_variable represent 'y' or the variable you want to predict\n",
    "        \n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        outputs,hidden_outputs = self.feedforward(inputs_list)\n",
    "        target = np.array(target_variable, ndmin=2).T\n",
    "        \n",
    "        #print(target_variable)\n",
    "        \n",
    "        # Find the error e = (t - o) - matrix subtraction \n",
    "        output_errors = target - outputs\n",
    "\n",
    "        # getting the hidden error ( W.T * e) - weights hidden - outputs\n",
    "        W_ho_T= (self.W_ho).T # transpose the weight matrix\n",
    "                \n",
    "        # find hidden layer error\n",
    "        hidden_errors = np.dot(W_ho_T,output_errors)\n",
    "        \n",
    "        # NOTE : FEED FORWARD already mapped the outputs to the sigmoid function\n",
    "        # implementing the delta weight change between the hidden layer and the output layer\n",
    "        \n",
    "        gradient = lambda x : x * (1-x)\n",
    "        \n",
    "        gradient_Who = gradient(outputs)\n",
    "        gradient_Wih = gradient(hidden_outputs)\n",
    "        \n",
    "        # link update between hidden layers and the output layers\n",
    "        self.W_ho += self.alpha * np.dot(gradient_Who, np.transpose(hidden_outputs))\n",
    "        \n",
    "        # link update between input layers and the hidden layers\n",
    "        self.W_ih += self.alpha * np.dot(gradient_Wih,np.transpose(inputs))\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # receive inputs -> generate hidden ! outputs\n",
    "    # feeding the information forward to the NN\n",
    "    def feedforward(self,inputs_list):\n",
    "        \n",
    "        # before predicting we need to turn the input into a 2D\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "    \n",
    "        \n",
    "    \n",
    "        # after that , we do a matrix multiplication of the inputs with the weights \n",
    "        # TODO add Bias\n",
    "        hidden_inputs = np.dot(self.W_ih,inputs)\n",
    "        # calculate the sigmoid function\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
    "        \n",
    "        # take the hidden outputs and do the matrix product between the hidden layer and the output layer\n",
    "\n",
    "        final_inputs = np.dot(self.W_ho,hidden_outputs)\n",
    "        \n",
    "\n",
    "        outputs = self.sigmoid(final_inputs)\n",
    "\n",
    "        \n",
    "        return outputs,hidden_outputs\n",
    "\n",
    "\n",
    "# number of input, hidden and output nodes\n",
    "# input_nodes =2\n",
    "# hidden_nodes = 2\n",
    "# output_nodes = 1\n",
    "# create instance of neural network\n",
    "\n",
    "#\n",
    "\n",
    "# n = neuralNetwork(2,2,2,0.5)\n",
    "\n",
    "# input_nodes = [1,0]\n",
    "# targets = [1,0]\n",
    "# n.train(input_nodes,targets)\n",
    "\n",
    "# class instance of the neural network       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data.\n",
    "X = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "# The labels for the training data.\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1]\n",
      "[1 0] [1]\n",
      "[1 1] [0]\n",
      "[0 0] [0]\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "for j in range(m):\n",
    "    print(X[j],y[j]) # 2x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52210622]]\n"
     ]
    }
   ],
   "source": [
    "n = neuralNetwork(2,2,1,0.01)\n",
    "\n",
    "#random.shuffle(training_data)\n",
    "# TODO : fix this damn thing\n",
    "for i in range(10):\n",
    "    for j in range(m):\n",
    "        n.train(X[j],y[j])\n",
    "        \n",
    "print(n.feedforward([0,0])[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "265a90865ad91d447cfa81a2dfc56a75477dab88c535fc37f696ed4f589bcec7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
