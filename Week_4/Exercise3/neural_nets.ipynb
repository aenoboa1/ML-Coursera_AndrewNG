{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is a Neural Network anyway?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset of dosages , ( low , medium and high), we can't fit a straight line to make an accurate prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/TcuaKgc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural network can effectively fit a squiggle for the data, even for complicated data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "- When you build a NN you have to decide what activation function you may use ( for learning : sigmoid , in practice : ReLU)\n",
    "- When you build a NN you make a guess on how many hidden layers and input you will use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this as an example of a NN :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://miro.medium.com/max/875/1*dypctO_eJnrXqX6JUa5MDA.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where a Hidden Layer can be defined as:\n",
    "\n",
    "- $ H*i = W*{ij} \\ * I\\_{i} + B$\n",
    "\n",
    "'W' represents the weight and is every connection between the input layer and the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Perceptron](https://en.wikipedia.org/wiki/Perceptron) can be defined as single layer NN , it's based on an artificial neuron, it can take several binary inputs $x_1,x_2,...,$ and produce a single binary output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Hidden node , the math processed is a 'weighted sum' , like so:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/hur6mHW.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork code and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a NN from scratch !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights , the heart of a NN\n",
    "\n",
    "The weights are one of the most important parameters of a NN, they are basically the link between the different layers of the NN , and they help feeding the error , calculating the prediction , and it's basically what helps improving the NN , the weight link can be represented as matrices , so we will use that to implement them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the input and hidden layers $W_{input \\ hiden}$ , of $ dim = (​\n",
    "  input_nodes​ \\ x\\ hidden_nodes ) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the hidden and the output layers $W_{hidden\\_output}$ , of $dim = (​\n",
    " output\\_nodes ​  \\ x\\ hidden\\_nodes )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the link weights should be small and\n",
    "random , unlike in logistic regression where we can simply set the initial weights as 0 , in a NN this will NOT work (if you initial the hidden units as 0 , then all of them will be symmetric matrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dw=\\begin{bmatrix}\n",
    "u & v \\\\\n",
    "u & v \n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both hidden units (eg: $a_1$ and $a_2$ ) will have the same influence on the output layer , after one iteration or many , both will remain symmetric , and no mather how many times you run it will compute the same function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn by refining their link weights. This is guided by the error​: the difference between the right answer given by the training data and their actual output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ending up in extreme parts of the correspoint function, it would be helpful to set $W$ as small numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36793829,  0.19483474, -0.3738566 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1,3)) - 0.5 # define also negative weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Gradient descent​ is a really good way of working out the minimum of a function, and it really works well when that function is so complex and difficult that we couldn’t easily\n",
    "work it out mathematically using algebra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the error as a matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ error*{hidden} = W^T * {hidden\\_output} \\ * error\\_{output} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = mx + b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $m$ to change based on the error :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta m = \\alpha x\\ * error $$\n",
    "\n",
    "$$ \\Delta B = \\alpha error $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation with gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is defined as a \" first-order iterative optimization algorithm for finding a local minimum of a differentiable function\", that is , in more human terms , a way of finding the steepest descent to a minimum .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Neural Networks , we are trying to minimize the NN error, defining the weight :\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W\\_{jk}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta W_{jk} = \\alpha E_k (O_k) + \\sigma(O_k) (1-\\sigma(O_k)) (O_j)^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us find the change in error as the weight links change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving a neural network means reducing this error ­ by changing those weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Choosing the right weights directly is too difficult. An alternative approach is to\n",
    "iteratively improve the weights by descending the <b> error </b> function, taking small steps.\n",
    "Each step is taken in the direction of the greatest downward slope from your current\n",
    "position. This is called ​ g\n",
    "radient descent​\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good recommendation is to rescale inputs into the range 0.0 to 1.0. Some will add a small\n",
    "offset to the inputs, like 0.01, just to avoid having zero inputs which are troublesome because\n",
    "they kill the learning ability by zeroing the weight update expression by setting that ​ oj=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should try to avoid saturating a NN keeping the inputs small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to actually propagate weights ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Bias Neural Network Class\n",
    "from scipy.special import expit  # expit reprents the sigmoid function\n",
    "\n",
    "class neuralNetwork:\n",
    "    #constructor body\n",
    "\n",
    "    def __init__(self,input_nodes,hidden_nodes,output_nodes,alpha):\n",
    "        \n",
    "        # defining a sigmoid or activation only once so that it can be referenced several times\n",
    "        self.sigmoid = lambda x: expit(x)\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # weights\n",
    "\n",
    "        self.W_ih =  np.array(np.random.random((self.hidden_nodes,self.input_nodes))) - 0.5 # needs to be array\n",
    "        self.W_ho = np.array(np.random.random((self.output_nodes,self.hidden_nodes))) - 0.5 \n",
    "\n",
    "\n",
    "        pass \n",
    "\n",
    "        \n",
    "    def train(self,inputs_list,target_variable): # target_variable represent 'y' or the variable you want to predict\n",
    "        \n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        outputs,hidden_outputs = self.feedforward(inputs_list)\n",
    "        target = np.array(target_variable, ndmin=2).T\n",
    "        \n",
    "        #print(target_variable)\n",
    "        \n",
    "        # Find the error e = (t - o) - matrix subtraction \n",
    "        output_errors = target - outputs\n",
    "\n",
    "        # getting the hidden error ( W.T * e) - weights hidden - outputs\n",
    "        W_ho_T= (self.W_ho).T # transpose the weight matrix\n",
    "                \n",
    "        # find hidden layer error\n",
    "        hidden_errors = np.dot(W_ho_T,output_errors)\n",
    "        \n",
    "        # NOTE : FEED FORWARD already mapped the outputs to the sigmoid function\n",
    "        # implementing the delta weight change between the hidden layer and the output layer\n",
    "        \n",
    "        gradient = lambda x : x * (1-x)\n",
    "        \n",
    "        gradient_Who = gradient(outputs)\n",
    "        gradient_Wih = gradient(hidden_outputs)\n",
    "        \n",
    "        # link update between hidden layers and the output layers\n",
    "        self.W_ho += self.alpha * np.dot(gradient_Who, np.transpose(hidden_outputs))\n",
    "        \n",
    "        # link update between input layers and the hidden layers\n",
    "        self.W_ih += self.alpha * np.dot(gradient_Wih,np.transpose(inputs))\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # receive inputs -> generate hidden ! outputs\n",
    "    # feeding the information forward to the NN\n",
    "    def feedforward(self,inputs_list):\n",
    "        \n",
    "        # before predicting we need to turn the input into a 2D\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "\n",
    "    \n",
    "        # after that , we do a matrix multiplication of the inputs with the weights \n",
    "        # TODO add Bias\n",
    "        hidden_inputs = np.dot(self.W_ih,inputs)\n",
    "        # calculate the sigmoid function\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
    "        \n",
    "        # take the hidden outputs and do the matrix product between the hidden layer and the output layer\n",
    "\n",
    "        final_inputs = np.dot(self.W_ho,hidden_outputs)\n",
    "        \n",
    "\n",
    "        outputs = self.sigmoid(final_inputs)\n",
    "\n",
    "        \n",
    "        return outputs,hidden_outputs\n",
    "\n",
    "\n",
    "# number of input, hidden and output nodes\n",
    "# input_nodes =2\n",
    "# hidden_nodes = 2\n",
    "# output_nodes = 1\n",
    "# create instance of neural network\n",
    "\n",
    "#\n",
    "\n",
    "# n = neuralNetwork(2,2,2,0.5)\n",
    "\n",
    "# input_nodes = [1,0]\n",
    "# targets = [1,0]\n",
    "# n.train(input_nodes,targets)\n",
    "\n",
    "# class instance of the neural network       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data.\n",
    "X = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "\n",
    "# The labels for the training data.\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12432/2604264221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "r = list(range(1000))\n",
    "random.shuffle(r)\n",
    "for i in r:\n",
    "    \n",
    "\n",
    "import random\n",
    "for i in range(5000):\n",
    "    for j in range(4):\n",
    "        x = random.choice(X[j])\n",
    "        print(x[j],y[j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99749076]]\n"
     ]
    }
   ],
   "source": [
    "n = neuralNetwork(2,2,1,0.01)\n",
    "\n",
    "#random.shuffle(training_data)\n",
    "# TODO : fix this damn thing\n",
    "for i in range(5000):\n",
    "    for j in range(4):\n",
    "        n.train(X[j],y[j])\n",
    "        \n",
    "print(n.feedforward([0,1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset of Handwritten Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-11-24 13:27:17--  https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 182023 (178K) [text/plain]\n",
      "Saving to: 'mnist_train_100.csv'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 28%  788K 0s\n",
      "    50K .......... .......... .......... .......... .......... 56% 1.86M 0s\n",
      "   100K .......... .......... .......... .......... .......... 84% 1.15M 0s\n",
      "   150K .......... .......... .......                         100% 4.81M=0.1s\n",
      "\n",
      "2021-11-24 13:27:18 (1.26 MB/s) - 'mnist_train_100.csv' saved [182023/182023]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/makeyourownneuralnetwork/makeyourownneuralnetwork/master/mnist_dataset/mnist_train_100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('mnist_train_100.csv','r') # r parameter is optional\n",
    "data_list = data_file.readlines() \n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15e83e31c40>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_value = data_list[0].split(',')\n",
    "\n",
    "img_array = np.asfarray(all_value[1:]).reshape(28,28) # return array convert to float type\n",
    "\n",
    "plt.imshow(img_array,cmap='inferno',interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the MNIST Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve worked out how to get data out of the MNIST data files and disentangle it so we can\n",
    "make sense of it, and visualise it too. We want to train our neural network with this data, but we\n",
    "need to think just a little about preparing this data before we throw it at our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to rescale the input color values from 0 to 255 , to a smaller range , say (0.01-1.0) avoiding 0 so they don't transform weights into ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = ( np.asfarray(all_value[1:]) )/ (255 * 0.99) + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.02188354 0.08130125 0.08130125 0.08130125\n",
      " 0.50910873 0.54872054 0.70320658 0.11299069 0.66755595 1.02010101\n",
      " 0.98841157 0.51306991 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.12883541 0.1526025  0.38235096 0.62002179\n",
      " 0.68340067 1.01217865 1.01217865 1.01217865 1.01217865 1.01217865\n",
      " 0.9012656  0.69132303 1.01217865 0.96860566 0.78243018 0.26351555\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.20409784\n",
      " 0.95276094 1.01217865 1.01217865 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 1.00425629 0.37838978 0.3348168\n",
      " 0.3348168  0.2318261  0.16448604 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.08130125 0.87749851 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 1.01217865 0.79431373 0.73093484\n",
      " 0.98841157 0.96464448 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.32689443 0.62794415 0.43384631 1.01217865\n",
      " 1.01217865 0.82204199 0.05357298 0.01       0.18033076 0.62002179\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.06545653 0.01396118 0.62002179 1.01217865 0.36650624\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.56060408 1.01217865 0.76262428 0.01792236 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.05357298\n",
      " 0.76262428 1.01217865 0.28728263 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.14864132 0.96464448\n",
      " 0.9012656  0.64378887 0.43780749 0.01396118 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.33085561 0.9606833  1.01217865\n",
      " 1.01217865 0.48138047 0.10902951 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.18825312 0.74677956 1.01217865 1.01217865\n",
      " 0.60417706 0.11695187 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.07337889 0.37838978 1.00821747 1.01217865 0.75074074\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.99633393 1.01217865 0.99633393 0.26351555 0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.1922143  0.52495346 0.73489602 1.01217865\n",
      " 1.01217865 0.82996435 0.01792236 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.16448604 0.5962547\n",
      " 0.91711032 1.01217865 1.01217865 1.01217865 1.00029511 0.73093484\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.10506833 0.46157457 0.88542088 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 0.80619727 0.31897207 0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.10110715 0.27143791 0.85373143 1.01217865\n",
      " 1.01217865 1.01217865 1.01217865 0.79431373 0.33085561 0.01792236\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.08130125 0.68736185\n",
      " 0.87749851 1.01217865 1.01217865 1.01217865 1.01217865 0.78243018\n",
      " 0.32689443 0.04565062 0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.22786492 0.69132303 0.90522678 1.01217865 1.01217865 1.01217865\n",
      " 1.01217865 0.97652803 0.536837   0.05357298 0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.54872054 1.01217865\n",
      " 1.01217865 1.01217865 0.84977025 0.54475936 0.53287582 0.07337889\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01       0.01       0.01\n",
      " 0.01       0.01       0.01       0.01      ]\n"
     ]
    }
   ],
   "source": [
    "print(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a label '5' the output should be:\n",
    "\n",
    "Example \"5\" : \n",
    "`\n",
    "[0.01, \n",
    "0.01,\n",
    "0.01,\n",
    "0.01,\n",
    "0.99,\n",
    "0.01,\n",
    "0.01,\n",
    "001,\n",
    "0.01,\n",
    "0.01]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the target matrix:\n",
    "\n",
    "output_nodes=10\n",
    "\n",
    "target = np.zeros(output_nodes) + 0.01\n",
    "target[int(all_value[0])] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, 0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "265a90865ad91d447cfa81a2dfc56a75477dab88c535fc37f696ed4f589bcec7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
