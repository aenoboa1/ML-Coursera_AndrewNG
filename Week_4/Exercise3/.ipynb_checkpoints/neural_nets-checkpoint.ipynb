{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Networks Definition \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what is a Neural Network anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset of dosages , ( low , medium and high), we can't fit a straight line to make an accurate prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/TcuaKgc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural network can effectively fit a squiggle for the data, even for complicated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition \n",
    "  \n",
    "\n",
    "- When you build a NN you have to decide what activation function you may use ( for learning : sigmoid , in practice : ReLU)\n",
    "- When you build a NN you make a guess on how many hidden layers and input you will use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this as an example of a NN :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://miro.medium.com/max/875/1*dypctO_eJnrXqX6JUa5MDA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where a Hidden Layer can be defined as:\n",
    "\n",
    "- $ H_i =  W_{ij} * I_{i}  + B$\n",
    "\n",
    "'W' represents the weight and is every connection between the input layer and the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Perceptron](https://en.wikipedia.org/wiki/Perceptron) can be defined as single layer NN , it's based on an artificial neuron, it can take several binary inputs $x_1,x_2,...,$ and produce a single binary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Hidden node , the math processed is a 'weighted sum' , like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](https://i.imgur.com/hur6mHW.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork code and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a NN from scratch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights , the heart of a NN\n",
    "\n",
    "The weights are one of the most important parameters of a NN, they are basically the link between the different layers of the NN , and they help feeding the error , calculating the prediction , and it's basically what helps improving the NN , the weight link can be represented as matrices , so we will use that to implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the input and hidden layers $W_{input \\ hiden}$ , of $ dim = (​\n",
    " input\\_nodes​ \\ x\\  hidden\\_nodes ) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix for the weights for links between the hidden and  the output layers $W_{hidden\\_output}$ , of $dim = (​\n",
    " output\\_nodes ​  \\ x\\ hidden\\_nodes )$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the link weights should be small and\n",
    "random , unlike in logistic regression where we can simply set the initial weights as 0 , in a NN this will NOT work (if you initial the hidden units as 0 , then all of them will be symmetric matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$dw=\\begin{bmatrix}\n",
    "u & v \\\\\n",
    "u & v \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both hidden units (eg: $a_1$ and $a_2$ ) will have the same influence on the output layer , after one iteration or many , both will remain symmetric , and no mather how many times you run it will compute the same function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks learn by refining their link weights. This is guided by the error​: the difference between the right answer given by the training data and their actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ending up in extreme parts of the correspoint function, it would be helpful to set $W$ as small numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4472479 , -0.00144761,  0.21367992]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1,3)) - 0.5 # define also negative weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    "Gradient descent​ is a really good way of working out the minimum of a function, and it really works well when that function is so complex and difficult that we couldn’t easily\n",
    "work it out mathematically using algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the error as a matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ error_{hidden} = W^T_{hidden\\_output} * error_{output} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = mx + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $m$ to change based on the error :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta m = \\alpha x* error $$ \n",
    "$$ \\Delta B = \\alpha  error $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation  with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is defined as a \" first-order iterative optimization algorithm for finding a local minimum of a differentiable function\", that is , in more human terms , a way of finding the steepest descent to a minimum .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Neural Networks , we are trying to minimize the NN error, defining the weight :\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial W_{jk}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Delta W_{jk} = \\alpha E_k (O_k) + \\sigma(O_k) (1-\\sigma(O_k)) (O_j)^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help us find the change in error as the weight links change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Improving a neural network means reducing this error ­ by changing those weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●\n",
    " Choosing the right weights directly is too difficult. An alternative approach is to\n",
    "iteratively improve the weights by descending the <b> error </b> function, taking small steps.\n",
    "Each step is taken in the direction of the greatest downward slope from your current\n",
    "position. This is called ​ g\n",
    "radient descent​\n",
    " ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good recommendation is to rescale inputs into the range 0.0 to 1.0. Some will add a small\n",
    "offset to the inputs, like 0.01, just to avoid having zero inputs which are troublesome because\n",
    "they kill the learning ability by zeroing the weight update expression by setting that ​ oj=0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We should try to avoid saturating a NN keeping the inputs small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to actually propagate weights ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.special import expit  # expit reprents the sigmoid function\n",
    "\n",
    "class neuralNetwork:\n",
    "    #constructor body\n",
    "    def __init__(self,input_nodes,hidden_nodes,output_nodes,alpha):\n",
    "        # defining a sigmoid or activation only once so that it can be referenced several times\n",
    "        self.sigmoid = lambda x: expit(x)\n",
    "\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "        # weights \n",
    "        self.W_ih = np.matrix(np.random.random((self.hidden_nodes,self.input_nodes))) - 0.5\n",
    "        self.W_ho = np.matrix(np.random.random((self.output_nodes,self.hidden_nodes))) - 0.5\n",
    "\n",
    "        pass \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def train(self,inputs_list,target_variable): # target_variable represent 'y' or the variable you want to predict\n",
    "\n",
    "        outputs = self.feedforward(inputs_list)\n",
    "\n",
    "        target = np.array(target_variable, ndmin=2).T\n",
    "\n",
    "\n",
    "        # Find the error e = (t - o) - matrix subtraction \n",
    "        output_errors = target - outputs\n",
    "\n",
    "        # getting the hidden error ( W.T * e) - weights hidden - outputs\n",
    "        W_ho_T= (self.W_ho).T # transpose the weight matrix\n",
    "        \n",
    "        \n",
    "        # find hidden layer error\n",
    "\n",
    "        hidden_errors = np.dot(W_ho_T,output_errors)\n",
    "        \n",
    "        # NOTE : FEED FORWARD already mapped the ouputs to the sigmoid function\n",
    "        # implementing the delta weight change between the hidden layer and the output layer\n",
    "        \n",
    "        print(outputs.shape)\n",
    "                \n",
    "        self.W_ho += self.alpha * np.dot((output_errors * outputs * (1.0 - outputs)), np.transpose(hidden_outputs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # receive inputs -> generate hidden ! outputs\n",
    "    # feeding the information forward to the NN\n",
    "    def feedforward(self,inputs_list):\n",
    "\n",
    "        # before predicting we need to turn the input into a 2D\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        # after that , we do a matrix multiplication of the inputs with the weights \n",
    "        # TODO add Bias\n",
    "        hidden_inputs = np.dot(self.W_ih,inputs)\n",
    "        # calculate the sigmoid function\n",
    "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
    "        \n",
    "        # take the hidden outputs and do the matrix product between the hidden layer and the output layer\n",
    "\n",
    "        final_inputs = np.dot(self.W_ho,hidden_outputs)\n",
    "        \n",
    "        outputs = self.sigmoid(final_inputs)\n",
    "\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# number of input, hidden and output nodes\n",
    "# input_nodes =2\n",
    "# hidden_nodes = 2\n",
    "# output_nodes = 1\n",
    "# create instance of neural network\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "# class instance of the neural network       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14816/1675347810.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minput_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14816/162791922.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inputs_list, target_variable)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_ho\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_errors\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__rmul__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "n = neuralNetwork(2,2,2,0.5)\n",
    "\n",
    "input_nodes = [1,0]\n",
    "targets = [1,0]\n",
    "n.train(input_nodes,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.diff(a, n=1, axis=-1, prepend=<no value>, append=<no value>)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training to XOR OR OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39163834],\n",
       "       [0.52415167],\n",
       "       [0.50979665]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(np.random.random((1,3)) , ndmin=2).T\n",
    "inputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "265a90865ad91d447cfa81a2dfc56a75477dab88c535fc37f696ed4f589bcec7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
